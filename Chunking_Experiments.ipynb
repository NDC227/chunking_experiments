{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scipy\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jK_FH2Y8h5K_",
        "outputId": "62a8c76b-31d2-44cf-80e8-9b2d72c313bd"
      },
      "outputs": [],
      "source": [
        "#from langchain_community.document_loaders import WikipediaLoader\n",
        "#docs = WikipediaLoader(query='Barack Obama', load_max_docs=2, doc_content_chars_max=100000).load()\n",
        "#print(len(docs))\n",
        "# print(docs[0].metadata)\n",
        "# print(docs[0].page_content)\n",
        "from datasets import load_dataset\n",
        "ds = load_dataset('cjlovering/natural-questions-short')\n",
        "class DocWrapper(object):\n",
        "  def __init__(self, s):\n",
        "    self.page_content = s['contexts']\n",
        "    self.metadata = {}\n",
        "docs = [DocWrapper(ex) for ex in ds['train']]\n",
        "print(docs[:10])\n",
        "print(docs[0].page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "create_chunks = True\n",
        "if os.path.isfile('data/chunks.json'):\n",
        "    create_chunks = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter, TokenTextSplitter, CharacterTextSplitter, NLTKTextSplitter,\\\n",
        "                                     SentenceTransformersTokenTextSplitter, SpacyTextSplitter\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "if create_chunks:\n",
        "    def get_splits(splitter, data):\n",
        "        return [split.page_content for split in splitter.split_documents(docs)]\n",
        "\n",
        "    rec_char_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
        "    )\n",
        "    rec_char_splitter_2 = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=2000, chunk_overlap=200, add_start_index=True\n",
        "    )\n",
        "    token_splitter = TokenTextSplitter(\n",
        "        encoding_name='gpt2', chunk_size=100, chunk_overlap=0\n",
        "    )\n",
        "    char_splitter = CharacterTextSplitter(\n",
        "        separator='\\n', is_separator_regex=False\n",
        "    )\n",
        "    nltk_splitter = NLTKTextSplitter(\n",
        "        separator='\\n\\n', language='english'\n",
        "    )\n",
        "    sentence_transformer_splitter = SentenceTransformersTokenTextSplitter(\n",
        "        chunk_overlap=50, model_name='sentence-transformers/all-mpnet-base-v2', tokens_per_chunk=None\n",
        "    )\n",
        "    spacy_splitter = SpacyTextSplitter(\n",
        "        separator='\\n\\n', pipeline='en_core_web_sm', max_length=1000000\n",
        "    )\n",
        "    semantic_chunker = SemanticChunker(\n",
        "        embeddings=HuggingFaceEmbeddings(), buffer_size=1, add_start_index=True\n",
        "    )\n",
        "    #... add more, like the SemanticChunker, etc.                    # DONE\n",
        "    #... use different parameters for these (especially chunk sizes) # TODO: How wide of a parameter space should we use? Default is 4000/200"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZAzrIOgiVp4",
        "outputId": "bda03448-1ce0-499b-e2a0-cfc391889c2d"
      },
      "outputs": [],
      "source": [
        "if create_chunks:\n",
        "    all_splits, all_ids = [], []\n",
        "    splitters = [rec_char_splitter, rec_char_splitter_2, token_splitter, char_splitter, nltk_splitter, \\\n",
        "                sentence_transformer_splitter]\n",
        "    splitters = [rec_char_splitter, token_splitter]\n",
        "    for splitter in splitters:\n",
        "        if splitter.__class__.__name__ != 'SemanticChunker':\n",
        "            print(f'{splitter.__class__.__name__}_{splitter._chunk_size}_{splitter._chunk_overlap}')\n",
        "        else:\n",
        "            print(f'{splitter.__class__.__name__}')\n",
        "        splits = get_splits(splitter, docs)\n",
        "        all_splits.extend(splits)\n",
        "        all_ids.extend([f'id_{splitter.__class__.__name__}_{ii}' for ii in range(len(splits))])\n",
        "    \n",
        "    df = pd.DataFrame.from_dict({'chunk': all_splits, 'id': all_ids})\n",
        "    df.to_json('data/chunks.json')\n",
        "else:\n",
        "    df = pd.read_json('data/chunks.json')\n",
        "    all_splits = list(df['chunk'].values)\n",
        "    all_ids = list(df['id'].values)\n",
        "    \n",
        "print(all_splits[0])\n",
        "print(len(all_splits))\n",
        "print(all_splits[:5])\n",
        "# print(all_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# from scipy import sparse\n",
        "\n",
        "\n",
        "# class BM25(object):\n",
        "#     def __init__(self, b=0.75, k1=1.6):\n",
        "#         self.vectorizer = TfidfVectorizer(norm=None, smooth_idf=False)\n",
        "#         self.b = b\n",
        "#         self.k1 = k1\n",
        "\n",
        "#     def fit(self, X):\n",
        "#         ''' Fit IDF to documents X '''\n",
        "#         self.vectorizer.fit(X)\n",
        "#         y = super(TfidfVectorizer, self.vectorizer).transform(X)\n",
        "#         self.avdl = y.sum(1).mean()\n",
        "\n",
        "#     def transform(self, q, X):\n",
        "#         ''' Calculate BM25 between query q and documents X '''\n",
        "#         b, k1, avdl = self.b, self.k1, self.avdl\n",
        "\n",
        "#         # apply CountVectorizer\n",
        "#         X = super(TfidfVectorizer, self.vectorizer).transform(X)\n",
        "#         len_X = X.sum(1).A1\n",
        "#         q, = super(TfidfVectorizer, self.vectorizer).transform([q])\n",
        "#         assert sparse.isspmatrix_csr(q)\n",
        "\n",
        "#         # convert to csc for better column slicing\n",
        "#         X = X.tocsc()[:, q.indices]\n",
        "#         denom = X + (k1 * (1 - b + b * len_X / avdl))[:, None]\n",
        "#         # idf(t) = log [ n / df(t) ] + 1 in sklearn, so it need to be coneverted\n",
        "#         # to idf(t) = log [ n / df(t) ] with minus 1\n",
        "#         idf = self.vectorizer._tfidf.idf_[None, q.indices] - 1.\n",
        "#         numer = X.multiply(np.broadcast_to(idf, X.shape)) * (k1 + 1)                                                          \n",
        "#         return (numer / denom).sum(1).A1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from fast_bm25 import BM25\n",
        "my_splits = [x.split() for x in all_splits]\n",
        "bm25 = BM25(my_splits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results = bm25.get_top_n(['largest', 'city', 'in', 'Japan'], my_splits, n=10)\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# bm25 = BM25()\n",
        "# bm25.fit(all_splits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def retrieve_from_query(query, k):\n",
        "    tokenized_query = query.split()\n",
        "    retrieve = bm25.get_top_n(tokenized_query, my_splits, n=k)\n",
        "    retrieved_docs = [' '.join(x) for x in retrieve]\n",
        "    return retrieved_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DOkKhXfNiWsx"
      },
      "outputs": [],
      "source": [
        "# import chromadb\n",
        "# chroma_client = chromadb.Client()\n",
        "# try:\n",
        "#     collection = chroma_client.create_collection(name='my_collection')\n",
        "# except:\n",
        "#     chroma_client.delete_collection(name='my_collection')\n",
        "#     collection = chroma_client.create_collection(name='my_collection')\n",
        "# collection.add(\n",
        "#     documents=all_splits[:100],\n",
        "#     ids=all_ids[:100]\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnA7uj7xoQto",
        "outputId": "4dffe237-ab89-4d70-e80a-2696784b6c08"
      },
      "outputs": [],
      "source": [
        "# results = collection.query(\n",
        "#     query_texts=['where is Obama from'],\n",
        "#     n_results=2 # how many results to return\n",
        "# )\n",
        "# print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPPx_TJGnhmN"
      },
      "outputs": [],
      "source": [
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "batch_size = 8\n",
        "tokenizer = AutoTokenizer.from_pretrained('facebook/opt-125m')\n",
        "\n",
        "# Now pass the retrieval results to the LLM (basically, RAG with frozen components)\n",
        "\n",
        "# Baseline: deduplicate retrieval results somehow\n",
        "\n",
        "# The neural net we train:\n",
        "# - For each question:\n",
        "#    - Encode each chunk and output a score (how to encode?)\n",
        "#    - Turn all scores over all chunks into a distribution P(d|q)\n",
        "#    - For each chunk get the NLL of the correct answer as another distribution Q(d|q) (from a RAG 'open domain QA' dataset; use HF transformers to get the NLL)\n",
        "#    - Minimize the KL div of those two distributions (KL_div(P||Q)) (https://arxiv.org/abs/2301.12652 - REPLUG)\n",
        "# - Add loss term for the length of the sequence (number of tokens)\n",
        "# - Train\n",
        "\n",
        "# For Chroma\n",
        "# def add_retrieval_results(ex):\n",
        "#     ex['retrieved'] = collection.query(\n",
        "#         query_texts=[ex['questions'][0]['input_text']],\n",
        "#         n_results=10 # the higher the better\n",
        "#     )['documents']\n",
        "#     # one alternative here for speed could be to not use Chroma but some kind of sparse search like Elastic\n",
        "#     return ex\n",
        "\n",
        "# For (fast) BM25\n",
        "def add_retrieval_results(ex):\n",
        "    ex['retrieved'] = retrieve_from_query(ex['questions'][0]['input_text'], k=10)\n",
        "    # one alternative here for speed could be to not use Chroma but some kind of sparse search like Elastic\n",
        "    return ex\n",
        "\n",
        "# Data pre-processing (tokenize and de-nesting)\n",
        "def preprocess(ex):\n",
        "    ex['questions'] = ex['questions'][0]['input_text']\n",
        "    ex['answers'] = ex['answers'][0]['span_text']\n",
        "\n",
        "    # tokenized_query = tokenizer(ex['questions'], padding='max_length', truncation=True, max_length=64, return_tensors='pt')\n",
        "    # ex['input'] = tokenized_query\n",
        "\n",
        "    # tokenized_docs = [tokenizer(doc, padding='max_length', truncation=True, max_length=100, return_tensors='pt') for doc in ex['retrieved']]\n",
        "    # ex['tokenized_docs'] = tokenized_docs\n",
        "\n",
        "    # tokenized_answer = tokenizer(ex['answers'], padding='max_length', truncation=True, max_length=16, return_tensors='pt')\n",
        "    # ex['tokenized_answer'] = tokenized_answer\n",
        "    return ex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('1')\n",
        "tiny_train_dataset = Dataset.from_dict(ds['train'][:])\n",
        "print('2')\n",
        "tiny_train_dataset = tiny_train_dataset.map(add_retrieval_results)\n",
        "print('3')\n",
        "tiny_train_dataset = tiny_train_dataset.map(preprocess)\n",
        "print('4')\n",
        "tiny_train_dataset = tiny_train_dataset.remove_columns(['contexts', 'has_correct_context', 'name', 'id'])\n",
        "tiny_train_loader = DataLoader(tiny_train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "tiny_valid_dataset = Dataset.from_dict(ds['validation'][:])\n",
        "tiny_valid_dataset = tiny_valid_dataset.map(add_retrieval_results)\n",
        "tiny_valid_dataset = tiny_valid_dataset.map(preprocess)\n",
        "tiny_valid_loader = DataLoader(tiny_valid_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# train_dataset = ds['train'].map(add_retrieval_results)\n",
        "# train_dataset = train_dataset.map(tokenize)\n",
        "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "# valid_dataset = ds['valid'].map(add_retrieval_results)\n",
        "# valid_dataset = valid_dataset.map(tokenize)\n",
        "# valid_loader = DataLoader(valid_dataset, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(tiny_train_dataset[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# print(tiny_train_dataset)\n",
        "# print(tiny_train_dataset['retrieved'])\n",
        "# print(tiny_train_dataset['input'])\n",
        "# print(tiny_train_dataset['tokenized_docs'])\n",
        "# print(tiny_train_dataset[0])\n",
        "# print(tiny_train_dataset['questions'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch = next(iter(tiny_train_loader))\n",
        "print(batch['questions'])\n",
        "# print(batch['retrieved'][0])\n",
        "# print(batch['input']['input_ids'][0])\n",
        "print(batch['answers'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "print((batch['retrieved']))\n",
        "print([x for retr in batch['retrieved'] for x in retr])\n",
        "print(tokenizer([x for retr in batch['retrieved'] for x in retr], padding=True, return_tensors='pt'))\n",
        "tokenized = tokenizer([x for retr in batch['retrieved'] for x in retr], padding=True, return_tensors='pt')\n",
        "print(tokenized['input_ids'].shape)\n",
        "tokenized['input_ids'] = torch.reshape(tokenized['input_ids'], (len(batch['retrieved'][0]), len(batch['retrieved']), -1))\n",
        "tokenized['attention_mask'] = torch.reshape(tokenized['attention_mask'], (len(batch['retrieved'][0]), len(batch['retrieved']), -1))\n",
        "print(tokenized['input_ids'].shape)\n",
        "# print(tokenizer(batch['retrieved']))\n",
        "# print([tokenizer(x, padding=True, return_tensors='pt') for x in batch['retrieved']])\n",
        "# torch.stack([tokenizer(x, padding=True, return_tensors='pt')['input_ids'] for x in batch['retrieved']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import wandb\n",
        "wandb.init(\n",
        "    # set the wandb project where this run will be logged\n",
        "    project='chunking_experiments',\n",
        "\n",
        "    # track hyperparameters and run metadata\n",
        "    config={\n",
        "    'learning_rate': 0.1,\n",
        "    'architecture': 'Transformer',\n",
        "    'dataset': 'NQ',\n",
        "    'epochs': 10,\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoModel\n",
        "model_id = 'facebook/opt-125m'\n",
        "llm_model = AutoModelForCausalLM.from_pretrained(model_id)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.emb_dim = 256\n",
        "        self.dim_ff = 1024\n",
        "        self.n_head = 4\n",
        "        self.n_layers = 4\n",
        "        self.embedding = nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=self.emb_dim)\n",
        "        self.encoding_layer = nn.TransformerEncoderLayer(d_model=self.emb_dim, nhead=self.n_head, dim_feedforward=self.dim_ff, batch_first=True)\n",
        "        self.encoding_block = nn.TransformerEncoder(self.encoding_layer, num_layers=self.n_layers)\n",
        "\n",
        "    def forward(self, input, mask):\n",
        "        x = self.embedding(input)\n",
        "        x = self.encoding_block(x, src_key_padding_mask=mask)\n",
        "        # x = self.encoding_block(x)\n",
        "        # print('before mean', x.shape, x)\n",
        "        x = torch.mean(x, dim=1)\n",
        "        # print('after mean', x.shape, x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import lightning as L\n",
        "from tqdm import tqdm\n",
        "\n",
        "from lightning.pytorch.demos import Transformer\n",
        "\n",
        "# TODO by Andrew: fill in the rest from here: https://lightning.ai/docs/pytorch/stable/common/lightning_module.html\n",
        "class ReplugTransformer(L.LightningModule):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # self.encoder = Transformer(vocab_size=vocab_size)\n",
        "        self.encoder = Encoder(vocab_size)\n",
        "        self.llm = llm_model  # LLM to get NLLs for reference distribution in KL div #TODO: separate \n",
        "        for param in self.llm.parameters():\n",
        "            param.requires_grad = False\n",
        "    \n",
        "    def forward(self, questions, docs):\n",
        "        questions, questions_mask = questions['input_ids'], torch.logical_not(torch.tensor(questions['attention_mask'], dtype=torch.bool))\n",
        "        docs, docs_mask = docs['input_ids'], torch.logical_not(torch.tensor(docs['attention_mask'], dtype=torch.bool))\n",
        "        # Encode query and documents\n",
        "        print('questions', questions)\n",
        "        print('questions_mask', questions_mask)\n",
        "        # print(len(docs), query.shape, docs[0].shape)\n",
        "        print('docs', docs.shape, docs)\n",
        "        n_examples = questions.shape[0]\n",
        "        q_emb = self.encoder(questions, questions_mask)\n",
        "        print('q_emb', q_emb.shape, q_emb)\n",
        "        d_embs = []\n",
        "        # Squeeze and unsqueeze to pass batch of retrievals into encoder at once?\n",
        "        # Turn B x K x L to (BK) x L\n",
        "        B, K, L = docs.shape\n",
        "        print(B, K, L)\n",
        "        input_docs = torch.reshape(docs, (B * K, L))\n",
        "        input_docs_mask = torch.reshape(docs_mask, (B * K, -1))\n",
        "        print(input_docs.shape)\n",
        "        d_embs = self.encoder(input_docs, input_docs_mask)\n",
        "        d_embs = torch.reshape(d_embs, (B, K, -1))\n",
        "        # for doc in docs:\n",
        "        #     # print(doc)\n",
        "        #     d_emb = self.encoder(doc)\n",
        "        #     d_embs.append(d_emb)\n",
        "        print(d_embs.shape)\n",
        "\n",
        "        d_scores = torch.einsum('bij,bjk->bik', d_embs, torch.unsqueeze(q_emb, -1))\n",
        "        d_scores = d_scores.squeeze()\n",
        "        print('d_scores', d_scores.shape, d_scores)\n",
        "\n",
        "        # # Calculate cosine similarity between query and all documents\n",
        "        # d_scores = []\n",
        "        # # print(len(d_embs))\n",
        "        # cos_sim = torch.nn.CosineSimilarity(dim=1)  #Dot product\n",
        "        # # print(len(docs), len(d_embs))\n",
        "        # # print(q_emb.shape)\n",
        "        # # print(docs[0].shape)\n",
        "        # # print(d_embs[0].shape)\n",
        "        # for d_emb in tqdm(d_embs):\n",
        "        #     # print(q_emb.shape, d_emb.shape)\n",
        "        #     similarity = cos_sim(q_emb, d_emb)\n",
        "        #     # print(similarity.shape)\n",
        "        #     d_scores.append(similarity)\n",
        "        # # print(d_scores)\n",
        "        # d_scores = torch.stack(d_scores, dim=0)\n",
        "        # # print(d_scores.shape)\n",
        "        # d_scores = torch.transpose(d_scores, 0, 1)\n",
        "        # # print(d_scores.shape)\n",
        "        # # print(d_scores)\n",
        "\n",
        "        return d_scores\n",
        "\n",
        "    # def llm_pass(self, questions, retrieved, answers):\n",
        "    #     # Run query through LLM with each chunk and get NLL\n",
        "    #     # with torch.no_grad():\n",
        "    #         llm_scores = []\n",
        "    #         for docs in tqdm(retrieved):\n",
        "    #             batch_scores = []\n",
        "    #             for i in range(len(questions)):\n",
        "    #                 q = questions[i]\n",
        "    #                 doc  = docs[i]\n",
        "    #                 answer = answers[i]\n",
        "    #                 # print(q)\n",
        "    #                 # print(doc + ' ' + q)\n",
        "    #                 # output = self.llm(doc + q)\n",
        "    #                 # loss = output.loss\n",
        "    #                 # llm_scores.append(loss)\n",
        "    #                 input = tokenizer(doc + ' ' + q, return_tensors='pt')\n",
        "    #                 # print(input['input_ids'])\n",
        "\n",
        "    #                 tokenized_answer = tokenizer(answer, return_tensors='pt')\n",
        "    #                 # print(tokenized_answer)\n",
        "    #                 probs = []\n",
        "    #                 tokenized_input = input['input_ids'].to('cuda')\n",
        "    #                 for token in tokenized_answer['input_ids'][0]:\n",
        "    #                     # print(input['input_ids'])\n",
        "    #                     expected_token = token.to('cuda')\n",
        "\n",
        "    #                     # print(next(self.llm.parameters()).is_cuda)\n",
        "    #                     # print(tokenized_input.is_cuda)\n",
        "    #                     outputs = self.llm(tokenized_input)\n",
        "    #                     tokenized_input = torch.cat((tokenized_input, torch.tensor([[expected_token]]).to('cuda')), 1)\n",
        "                        \n",
        "    #                     last_output = outputs['logits'][0][-1]\n",
        "    #                     last_output = torch.nn.functional.softmax(last_output, dim=0)\n",
        "    #                     # print(last_output)\n",
        "    #                     last_output = list(last_output.detach().cpu().numpy())\n",
        "\n",
        "    #                     last_idx = last_output.index(max(last_output))\n",
        "    #                     probs.append(last_output[expected_token])\n",
        "                        \n",
        "    #                     # print('expected prob: ', last_output[expected_token], ' token: ', tokenizer.decode(expected_token))\n",
        "    #                     # print(last_idx)\n",
        "    #                     # print(tokenizer.decode(last_idx))\n",
        "    #                 # print(probs)\n",
        "\n",
        "    #                 #perplexity\n",
        "    #                 denom = 1\n",
        "    #                 for prob in probs:\n",
        "    #                     denom *= (1 / prob)\n",
        "    #                 perplexity = denom ** (1 / len(probs))\n",
        "    #                 # print(perplexity)\n",
        "\n",
        "    #                 #ll\n",
        "    #                 total = 0\n",
        "    #                 for prob in probs:\n",
        "    #                     total += np.log10(prob)\n",
        "    #                 # print(total)\n",
        "    #                 # total *= -1\n",
        "    #                 score = total\n",
        "    #                 batch_scores.append(score)\n",
        "    #             # print(batch_scores)\n",
        "    #             llm_scores.append(batch_scores)\n",
        "    #         llm_scores = torch.tensor(llm_scores)\n",
        "    #         llm_scores = torch.transpose(llm_scores, 0, 1).to('cuda')\n",
        "    #         # print(llm_scores)\n",
        "    #         return llm_scores\n",
        "    \n",
        "    def new_llm_pass(self, questions, docs, answers):\n",
        "        # Again, reshape to put into LLM as B x ? shape\n",
        "        questions, questions_mask = questions['input_ids'], torch.logical_not(torch.tensor(questions['attention_mask'], dtype=torch.bool))\n",
        "        docs, docs_mask = docs['input_ids'], torch.logical_not(torch.tensor(docs['attention_mask'], dtype=torch.bool))\n",
        "        answers, answers_mask = answers['input_ids'], torch.logical_not(torch.tensor(answers['attention_mask'], dtype=torch.bool)) # B x A\n",
        "        B, K, L = docs.shape\n",
        "        docs = torch.reshape(docs, (B*K, L))                   # BK x L\n",
        "        _, answer_length = answers.shape\n",
        "        expanded_questions = torch.unsqueeze(questions, 1)                 # B x 1 x S\n",
        "        expanded_questions = expanded_questions.expand(-1, K, -1)          # B x K x S\n",
        "        expanded_questions = torch.reshape(expanded_questions, (B*K, -1))  # BK x S\n",
        "        print('expanded_questions', expanded_questions.shape, expanded_questions)\n",
        "        combined_input = torch.cat([docs, expanded_questions], dim=1) # BK x (S + L)\n",
        "        print('combined_input', combined_input.shape)\n",
        "        expanded_answers = torch.unsqueeze(answers, 1)                # B x 1 x A\n",
        "        expanded_answers = expanded_answers.expand(-1, K, -1)         # B x K x A\n",
        "        expanded_answers = torch.reshape(expanded_answers, (B*K, -1)) # BK x A\n",
        "        # expanded_answers_mask = torch.unsqueeze(answers_mask, 1)                # B x 1 x A\n",
        "        # expanded_answers_mask = expanded_answers_mask.expand(-1, K, -1)         # B x K x A\n",
        "        # expanded_answers_mask = torch.reshape(expanded_answers_mask, (B*K, -1)) # BK x A\n",
        "        all_scores = []                                          # BK x A\n",
        "        for i in range(answer_length):\n",
        "            expected_tokens = expanded_answers[:, i]             # BK x 1\n",
        "            print('expected_tokens', expected_tokens.shape, expected_tokens)\n",
        "            outputs = self.llm(combined_input)                   # BK x (S+L+i) x V\n",
        "            print('outputs[logits]', outputs['logits'].shape)\n",
        "            last_outputs = outputs['logits'][:, -1, :]           # BK x V\n",
        "            print('last_outputs', last_outputs.shape, last_outputs)\n",
        "            scores = last_outputs[torch.arange(B*K), expected_tokens] # BK x 1\n",
        "            print('scores', scores.shape, scores)\n",
        "            all_scores.append(scores)\n",
        "\n",
        "            combined_input = torch.cat([combined_input, torch.unsqueeze(expected_tokens, 1)], dim=1) #BK x (S + L + i)\n",
        "        all_scores = torch.stack([x for x in all_scores], dim=1)\n",
        "        print('all_scores', all_scores.shape, all_scores)\n",
        "        all_scores = all_scores.reshape(B, K, -1)\n",
        "        print('all_scores', all_scores.shape, all_scores)\n",
        "        all_scores = torch.mean(all_scores, dim=-1)\n",
        "        print('all_scores', all_scores.shape, all_scores)\n",
        "        return all_scores\n",
        "\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        # TODO: Make this actually work with the dataset constructed above\n",
        "        # Pre-Process data (moved from above in order to batch tokenize for efficiency)\n",
        "        questions = batch['questions']\n",
        "        docs = batch['retrieved']\n",
        "        answers = batch['answers']\n",
        "        tokenized_questions = tokenizer(questions, padding=True, return_tensors='pt').to('cuda')\n",
        "        tokenized_docs = tokenizer([x for retr in docs for x in retr], padding=True, return_tensors='pt').to('cuda')\n",
        "        tokenized_docs['input_ids'] = torch.reshape(tokenized_docs['input_ids'], (len(docs[0]), len(docs), -1))\n",
        "        tokenized_docs['attention_mask'] = torch.reshape(tokenized_docs['attention_mask'], (len(batch['retrieved'][0]), len(batch['retrieved']), -1))\n",
        "        tokenized_answers = tokenizer(answers, padding=True, return_tensors='pt').to('cuda')\n",
        "\n",
        "        # Normalize the retrieval scores from the forward pass\n",
        "        # TODO: Apply the masks in the scoring process - currently no masks but still converges on single batch\n",
        "        # print(batch['tokenized_docs'][0]['input_ids'])\n",
        "        # print(torch.stack([torch.stack(batch['tokenized_docs'][0]['input_ids'][i]) for i in range(len(batch['tokenized_docs'][0]['input_ids']))]))\n",
        "        # print(torch.transpose(torch.stack(batch['tokenized_docs'][0]['input_ids'][0], dim=0), 0, 1))\n",
        "        # print(torch.stack(batch['tokenized_docs']['input_ids'][0], dim=0))\n",
        "        # print(len(batch['tokenized_docs']))\n",
        "        # print(batch['tokenized_docs'][0]['input_ids'])\n",
        "        # tokenized_query = torch.transpose(torch.stack(batch['input']['input_ids'][0], dim=0), 0, 1)\n",
        "        # query_mask = torch.transpose(torch.stack(batch['input']['attention_mask'][0], dim=0), 0, 1)\n",
        "        # tokenized_docs = torch.stack([\n",
        "        #                     torch.stack([ex for ex in batch['tokenized_docs'][i]['input_ids']][0])\n",
        "        #                  for i in range(len(batch['tokenized_docs']))])\n",
        "        # tokenized_docs = torch.transpose(tokenized_docs, 1, 2)\n",
        "        # tokenized_docs = torch.transpose(tokenized_docs, 0, 1)\n",
        "        # docs_mask = torch.stack([\n",
        "        #                     torch.stack([ex for ex in batch['tokenized_docs'][i]['attention_mask']][0])\n",
        "        #                  for i in range(len(batch['tokenized_docs']))])\n",
        "        # docs_mask = torch.transpose(docs_mask, 1, 2)\n",
        "        # docs_mask = torch.transpose(docs_mask, 0, 1)\n",
        "        # tokenized_answer = torch.transpose(torch.stack(batch['tokenized_answer']['input_ids'][0], dim=0), 0, 1)\n",
        "        # answer_mask = torch.transpose(torch.stack(batch['tokenized_answer']['attention_mask'][0], dim=0), 0, 1)\n",
        "        # print('tokenized_docs', tokenized_docs.shape)\n",
        "        # print('tokenized_answer', tokenized_answer.shape, tokenized_answer)\n",
        "        # query_mask = torch.tensor(query_mask, dtype=torch.bool)\n",
        "        # docs_mask = torch.tensor(docs_mask, dtype=torch.bool)\n",
        "        # answer_mask = torch.tensor(answer_mask, dtype=torch.bool)\n",
        "        # print('query_mask', query_mask)\n",
        "        # questions = batch['questions']\n",
        "        # retrieved = batch['retrieved']\n",
        "        # answers = batch['answers']\n",
        "        # print(len(retrieved))\n",
        "        # print(tokenized_input.shape, tokenized_docs.shape)\n",
        "\n",
        "        reranker_output = self(tokenized_questions, tokenized_docs)  # output is retrieval scores?\n",
        "        rerank_dist = torch.nn.functional.log_softmax(reranker_output, dim=1)\n",
        "        \n",
        "        # Run an LLM to get the NLLs - NLLs or is it just the logits??\n",
        "        llm_output = self.new_llm_pass(tokenized_questions, tokenized_docs, tokenized_answers)\n",
        "        llm_dist = torch.nn.functional.softmax(llm_output, dim=1)\n",
        "\n",
        "        print('rerank', rerank_dist.shape, rerank_dist)\n",
        "        # print(rerank_dist)\n",
        "        print('llm', llm_dist.shape, llm_dist)\n",
        "        # print(llm_dist)\n",
        "\n",
        "        # Compute loss = kldiv(scores, nlls)\n",
        "        lossfn = torch.nn.KLDivLoss(reduction='batchmean')\n",
        "        loss = lossfn(rerank_dist, llm_dist) # see docs for notation https://pytorch.org/docs/stable/generated/torch.nn.KLDivLoss.html\n",
        "        print(loss)\n",
        "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True, batch_size=batch_size)\n",
        "        wandb.log({'train_loss': loss})\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.AdamW(self.encoder.parameters(), lr=1e-4)\n",
        "    \n",
        "    # def score_doc(self, query, doc, answer):\n",
        "    #     # with torch.no_grad():\n",
        "    #         input = tokenizer(doc + ' ' + query, return_tensors='pt')\n",
        "    #         # print(input['input_ids'])\n",
        "\n",
        "    #         tokenized_answer = tokenizer(answer, return_tensors='pt')\n",
        "    #         # print(tokenized_answer)\n",
        "    #         probs = []\n",
        "    #         tokenized_input = input['input_ids'].to('cuda')\n",
        "    #         for token in tokenized_answer['input_ids'][0]:\n",
        "    #             # print(input['input_ids'])\n",
        "    #             expected_token = token.to('cuda')\n",
        "\n",
        "    #             # print(next(self.llm.parameters()).is_cuda)\n",
        "    #             # print(tokenized_input.is_cuda)\n",
        "    #             outputs = self.llm(tokenized_input)\n",
        "    #             tokenized_input = torch.cat((tokenized_input, torch.tensor([[expected_token]]).to('cuda')), 1)\n",
        "                \n",
        "    #             last_output = outputs['logits'][0][-1]\n",
        "    #             last_output = torch.nn.functional.softmax(last_output, dim=0)\n",
        "    #             # print(last_output)\n",
        "    #             last_output = list(last_output.detach().cpu().numpy())\n",
        "\n",
        "    #             last_idx = last_output.index(max(last_output))\n",
        "    #             probs.append(last_output[expected_token])\n",
        "                \n",
        "    #             # print('expected prob: ', last_output[expected_token], ' token: ', tokenizer.decode(expected_token))\n",
        "    #             # print(last_idx)\n",
        "    #             # print(tokenizer.decode(last_idx))\n",
        "    #         # print(probs)\n",
        "\n",
        "    #         #perplexity\n",
        "    #         denom = 1\n",
        "    #         for prob in probs:\n",
        "    #             denom *= (1 / prob)\n",
        "    #         perplexity = denom ** (1 / len(probs))\n",
        "    #         # print(perplexity)\n",
        "\n",
        "    #         #ll\n",
        "    #         total = 0\n",
        "    #         for prob in probs:\n",
        "    #             total += np.log10(prob)\n",
        "    #         # print(total)\n",
        "    #         # total *= -1\n",
        "    #         return total\n",
        "\n",
        "model = ReplugTransformer(vocab_size=tokenizer.vocab_size)\n",
        "trainer = L.Trainer()\n",
        "trainer.fit(model, tiny_train_loader, tiny_valid_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#for batch in ds['train']:\n",
        "#  batch_scores = net(batch) # Bx1 float\n",
        "#  retrieval_scores = F.softmax(batch_scores)\n",
        "#  nll_scores = llm(batch).mean() # Bx1 float\n",
        "#  loss = kldiv(batch_scores, nll_scores)\n",
        "#  # TODO: loss += weight_coeff * sum([len(x) for x in batch])\n",
        "#  loss.backward()\n",
        "\n",
        "# Process:\n",
        "# - Retrieve with high n_results\n",
        "# - Rerank using the above neural net\n",
        "# - Take the top-k from that reranker\n",
        "# - Give to LLM\n",
        "# - Evaluate\n",
        "\n",
        "# Once the above actually works:\n",
        "# - Add additional deduplication steps?\n",
        "# - Add a diversity term to the loss?\n",
        "# - Add metadata to the chunk (e.g. prefix 'TokenTextSplitter' to each such chunk for the net() call, NOT the llm() call)\n",
        "\n",
        "\n",
        "# Document score side:\n",
        "# Input: BxS\n",
        "# After retrieval: (BxS, BxKxL) (assume tokenized)\n",
        "# After encoding: (BxSxE -> BxE), (BxKxE) (where E is embedding/hidden dim, separate encoders)\n",
        "# After scoring: BxK (do bmm/einsum to get this)\n",
        "# After normalize: BxK (logsoftmax, make sure you don't do double log)\n",
        "\n",
        "# NLL score side:\n",
        "# Input: BxS\n",
        "# After encoding: Bx(S+L) -> Bx(Correct Answer)xV -> select logit of correct token, take mean -> Bx1\n",
        "\n",
        "# 'Context: The largest city in Japan is Tokyo. Question: What is the largest city in Japan? Answer: ' -> what is the logit for Tokyo?\n",
        "# Autoregressively generate for max_length_of_correct_answers_in_batch, giving you some matrix BxM,\n",
        "# zero out all the components that you don't need (ie if the correct answer was 1 token, all the other tokens you can zero out),\n",
        "# then take the mean of the non-zero elements to get Bx1 NLL tensor\n",
        "\n",
        "# ^ TODO: double check that this is how Replug does it\n",
        "\n",
        "# After loss: Bx1 (kldiv)\n",
        "\n",
        "# Experimental protocol:\n",
        "# 1. Validate that you get the above shapes\n",
        "# 2. Print the BxK scores over time, make sure they change\n",
        "# 3. 'Spike' the retrievals with 7 bad examples and 1 correct one, make sure the mass shifts to the correct one\n",
        "# 4. Make sure train loss goes down (first on the spiked one, then on single batch, then all batches)\n",
        "# 5. Run with fast BM25\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
